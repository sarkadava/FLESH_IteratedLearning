---
title: "Gillian stats start"
date: "2024-03-25"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

# Introduction

Here we say what are we going to do, stating all questions in the most clear way possible

# Questions

Part 1
- q
- q 
- q

Part 2
- q 
- q
- q 


# Setting up the environment

```{r setup environment, echo=TRUE, message=FALSE, warning=FALSE}

library(here)
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)
library(tibble)
library(rcartocolor)


library(ggdag) # for dag
library(dagitty)


library(ggplot2) # bayesian stuff
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)

# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '\\Gillian_stats\\datasets\\') # NOTE THAT HERE YOU MIGHT NEED TO ADAPT A BIT
models        <- paste0(parentfolder, '\\Gillian_stats\\models\\')
plots         <- paste0(parentfolder, '\\Gillian_stats\\plots\\')


```


Make sure you use all available cores
```{r cores, include=FALSE, message=FALSE, warning=FALSE}

# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
```


# Loading in our data

```{r loading data, echo=TRUE, message=FALSE, warning=FALSE}

data <- read_csv(paste0(datasets, "unnormed_features.csv")) # THIS IS WHERE YOU LOAD IN YOUR FINAL DATA
```

Here you try to little bit look into structure of your data

- what is distribution of gender, age, extroversion, number of languages for example

Also here you want to make sure all columns are of types you need - so for example factorizing categorical variables etc.

```{r h1 factorizing, message=FALSE, warning=FALSE}

#data2$generation_number <- as.factor(data2$generation_number)
data$participantID <- as.factor(data$participantID)
data$chainID <- as.factor(data$chainID)
data$gesture <- as.factor(data$gesture)

```

# Question XX - gesture space

Here we start to focus on our first question

State your hypothesis, ie., gesture space decreases with generation

Here we look at how the question is answered purely by the data

```{r}

ggplot(data, aes(x = generation_number, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Generation Number", 
       y = "Total Gesture space (2D)", 
       title = "Linear Relationship between Generation Number and Gesture space") +
  theme_minimal()

```

Look at the distribution of the outcome

```{r}

hist(data$total_gesture_space2D)

```

## Data wrangling

To have easier interpretation of the data, we will center the generation number. With that, our estimate of the intercept (here gesture space) will always apply for generation in the middle. Otherwise, it would be estimate for the first generation

```{r}

data$generation_number <- as.numeric(data$generation_number)
data$generation_number_c <- data$generation_number - median(range(data$generation_number))

```

## Model 1 - intercept only

This is the model
```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m1.gs <- brm(
  total_gesture_space2D ~ generation_number_c, 
  data = data,
  family = gaussian(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m1.gs <- add_criterion(m1.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m1.gs_R2 <- bayes_R2(m1.gs)

# Save both as objects
saveRDS(m1.gs, file.path(models, "m1.gs.rds"))
saveRDS(m1.gs_R2, file.path(models, "m1.gs_R2.rds"))

beep(5)

```

Here we load the model so that we don't have to run the model again
```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m1.gs <- readRDS(file.path(models, "m1.gs.rds"))
m1.gs_R2 <- readRDS(file.path(models, "m1.gs_R2.rds"))


# Summary
summary(m1.gs)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m1.gs)
# make some comments of what you see - do you see nice hairy catterpillars?

plot(conditional_effects(m1.gs), points = TRUE)
# also here about what effect you see or not see

pp_check(m1.gs, type = "dens_overlay")
# how well does the model predict future data?

pp_check(m1.gs, type = "error_scatter_avg")
# how are the values correlated with residual error?

m1.gs_R2
# how much variance the model explains? 

```
Now we are not happy, because for example ppcheck reveals that...

As we inspected the outcome variable, we think that lognormal family could be maybe more appropriate. So we try it 

## Model 2 lognormal

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m2.gs <- brm(
  total_gesture_space2D ~ generation_number_c, 
  data = data,
  family = lognormal(),  # Here we change lognormal
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m2.gs <- add_criterion(m2.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m2.gs_R2 <- bayes_R2(m2.gs)

# Save both as objects
saveRDS(m2.gs, file.path(models, "m2.gs.rds"))
saveRDS(m2.gs_R2, file.path(models, "m2.gs_R2.rds"))


beep(5)

```
 
And we do the same procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m2.gs <- readRDS(file.path(models, "m2.gs.rds"))
m2.gs_R2 <- readRDS(file.path(models, "m2.gs_R2.rds"))


# Summary
summary(m2.gs)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m2.gs)
# comments

plot(conditional_effects(m2.gs), points = TRUE)
# comments

pp_check(m3.gs, type = "dens_overlay")
# comments

pp_check(m2.gs, type = "error_scatter_avg")
# comments

m2.gs_R2
# comments

```

We are still not acknowledging some structures of the data to the model. There is some variance coming from XYZ. So we want to add it into the model too

## Model 3 - lognormal with random effects

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m3.gs <- brm(
  total_gesture_space2D ~ generation_number_c + (1 | participantID) + (1 | gesture) + (1 | chainID), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m3.gs <- add_criterion(m3.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m3.gs_R2 <- bayes_R2(m3.gs)

# Save both as objects
saveRDS(m3.gs, file.path(models, "m2.gs.rds"))
saveRDS(m3.gs_R2, file.path(models, "m2.gs_R2.rds"))


beep(5)

```

And once again we do all over the whole procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m3.gs <- readRDS(file.path(models, "m3.gs.rds"))
m3.gs_R2 <- readRDS(file.path(models, "m3.gs_R2.rds"))


# Summary
summary(m3.gs)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m3.gs)
#

plot(conditional_effects(m3.gs), points = TRUE)
#

pp_check(m3.gs, type = "dens_overlay")
# 

pp_check(m3.gs, type = "error_scatter_avg")
# 


m3.gs_R2
# 

```


### Converting to original scale

We also have to remember that model with lognormal distribution outputs all coefficients (intercept, etc) on logscale, not the natural one

The following code converts the mean estimates to original scale

This is for all predictors (except concept and participant)
```{r h1.m6 converting5, message=FALSE, warning=FALSE}

# Extract posterior samples
posterior_samples <- as_draws_df(m3.gs)
alpha_samples <- posterior_samples$b_Intercept

# Initialize effect list
effect_list <- list()

# Helper function to calculate summary statistics
get_effect_summary <- function(effect_samples) {
  mean_effect <- mean(effect_samples)
  se_effect <- sd(effect_samples)
  ci_effect <- quantile(effect_samples, c(0.025, 0.975))
  post_prob <- mean(effect_samples > 0)
  c(mean = mean_effect, 
    se = se_effect, 
    lower_ci = ci_effect[1], 
    upper_ci = ci_effect[2], 
    post_prob = post_prob)
}

# GENERATION NUMBER (centered continuous)
if ("b_generation_number_c" %in% colnames(posterior_samples)) {
  beta_generation <- posterior_samples$b_generation_number_c
  effect_list$Generation_number_c <- get_effect_summary(exp(alpha_samples + beta_generation) - exp(alpha_samples))
}

# Convert to a nicely formatted data frame
effect_df <- do.call(rbind, effect_list)

# View effects
effect_df

```

## Diagnostics I

These are diagnostics that will help us to assess and validate each models in terms of their predictive power

```{r h1 diagnostics I, include=FALSE, message=FALSE, warning=FALSE}

model_list <- list() # here we list all the models

r2_list <- list() # here we list all r2 objects

```

### Rhat

```{r h1 rhat I, message=FALSE, warning=FALSE}

# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
  rhat_values <- rhat(model)
  data.frame(model = deparse(substitute(model)), 
             max_rhat = max(rhat_values), 
             min_rhat = min(rhat_values))
})

# Combine and inspect
do.call(rbind, rhat_list)

```


### ESS

Effective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix

```{r h1 ess I, message=FALSE, warning=FALSE}

# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
  neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
  data.frame(model = deparse(substitute(model)), 
             min_neff = min(neff_values), 
             max_neff = max(neff_values),
             mean_neff = mean(neff_values))
               
})

# Combine and inspect
do.call(rbind, neff_ratio_list)

```

Now you can look at the actuall ESS of some models

```{r h1 ess I.I, message=FALSE, warning=FALSE}

effective_sample(h1.m6c) 


```



### LOO & WAIC

Leave-one-out (loo) validation
```{r h1 loo I, message=FALSE, warning=FALSE}

l <- loo_compare(#here you list your models, criterion = "loo")

print(l, simplify = F)

```
elpd_loo: This is the expected log pointwise predictive density for LOO. Higher values indicate a better fit to the data.

se_elpd_loo: The standard error of the elpd_loo, representing uncertainty in the model’s predictive fit according to LOO.

looic: The LOO Information Criterion, which is similar to waic but based on leave-one-out cross-validation. Lower values are better.

p_loo: The effective number of parameters according to LOO, indicating the model’s complexity.

se_p_loo: The standard error of p_loo, representing uncertainty around the effective number of parameters.

So lognormal seems the best. 


Information criterion (WAIC)
```{r h1 waic I, message=FALSE, warning=FALSE}

w <- loo_compare(#here you list your models, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```
elpd_waic (expected log pointwise predictive density for WAIC): This represents the model's predictive fit to the data. Higher values indicate a better fit.

se_elpd_waic (standard error of elpd_waic): Measures uncertainty around the elpd_waic estimate.

waic: The Widely Applicable Information Criterion, a measure of model fit where lower values indicate a better fit.

se_waic (standard error of WAIC): Uncertainty around the WAIC estimate.

elpd_diff: The difference in the elpd_waic between the model in question and the baseline model (fit_eff_2, which has elpd_diff of 0). A negative value indicates that the model fits worse than fit_eff_2.

se_diff: The standard error of the elpd_diff, indicating how much uncertainty there is in the difference in predictive performance.

p_waic: The number of effective parameters in the model (related to model complexity). Lower values indicate simpler models, and higher values suggest more complexity.

Plot the comparison
```{r h1 waic plot, echo=FALSE, message=FALSE, warning=FALSE}

w[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column("model_name") %>% 
  mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %>% 
  
  ggplot(aes(x = waic, y = model_name, 
             xmin = waic - se_waic, 
             xmax = waic + se_waic)) +
  geom_pointrange(color = carto_pal(7, "BurgYl")[7], 
                  fill = carto_pal(7, "BurgYl")[5], shape = 21) +
  labs(title = "WAIC plot",
       x = NULL, y = NULL) +
  theme(axis.ticks.y = element_blank())

```

```{r h1 weights, message=FALSE, warning=FALSE}

model_weights(#here again you list your models, weights = "waic") %>% 
  round(digits = 2)

```


From this, you should be more confident in saying, we choose this model and the model says there is XY effect.

You can reuse the same model for the rest of the features - as long as you are within one question type (so here relation between a feature and generation)

Now onto next question

# Questions about age-gesture space etc.


# Questions about distance (rate of evolution)
