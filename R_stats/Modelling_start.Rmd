---
title: "Gillian stats start"
date: "2024-04-18"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

# Introduction

In this stats file, we answer questions about the iterated learning experiment conducted on the Long Night of Sciences.

# Questions

Part 1
- Does generation have an effect on gesture space used?
- Does generation have an effect on entropy used?
- Does generation have an effect on temporal variability used?
- Does Age and Extroversion have an effect on gesture space, enotrpy, and temporal variability? (See Model Four)

For part one, we test four different models for each of the first three questions. 
Model one uses a gaussian distribution and looks only at intercepts.
Model two uses a lognormal distribution and looks only at intercepts.
Model three uses a lognormal distribution and includes random effects
Model four uses a lognormal distribution and includes random effects and fixed effects of age and extroversion.

Part 2
- Does does a bigger diff in age mean greater distance between one gesture and another?
- Does does a bigger diff in extroversion mean greater distance between one gesture and another?
- Does does a bigger diff in num of languages mean greater distance between one gesture and another?


# Setting up the environment

```{r setup environment, echo=TRUE, message=FALSE, warning=FALSE}

library(here)
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)
library(tibble)
library(rcartocolor)

library(ggdag) # for dag
library(dagitty)

library(ggplot2) # bayesian stuff
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)

# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '/Gillian_stats/datasets/') # This line is different on mac and others
models        <- paste0(parentfolder, '/Gillian_stats/models/')
plots         <- paste0(parentfolder, '/Gillian_stats/plots/')


```

# use all available cores
```{r cores, include=FALSE, message=FALSE, warning=FALSE}

# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
```
# Loading in our data

```{r loading_data, echo=TRUE, message=FALSE, warning=FALSE}

data <- read_csv(paste0(datasets, "final_stats_dataframe.csv"))
```

Here we look into structure of our data: what is distribution of gender, age, extroversion, number of languages for example

```{r h1 visualizing data, message=FALSE, warning=FALSE}
hist(data$Extroversion)
hist(data$Age)
hist(data$TotalLanguages)
```

Also here you want to make sure all columns are of types you need - so for example factorizing categorical variables etc.

```{r h1 factorizing, message=FALSE, warning=FALSE}

#data2$generation_number <- as.factor(data2$generation_number)
data$participantID <- as.factor(data$participantID)
data$chain_number <- as.factor(data$chain_number)

data$gesture <- as.factor(data$gesture)
data$Gender <- as.factor(data$Gender)

data$Age <- as.numeric(data$Age)
data$Extroversion <- as.numeric(data$Extroversion)
data$NumOfNativeLanguages <- as.numeric(data$NumOfNativeLanguages)

data$generation_number <- as.numeric(data$generation_number)
data$total_distance <- as.numeric(data$total_distance)
data$diff_in_age <- as.numeric(data$diff_in_age)
data$diff_in_extrovet <- as.numeric(data$diff_in_extrovet)
data$diff_in_gender <- as.factor(data$diff_in_gender)
data$diff_in_TotalNativeLanguages <- as.numeric(data$diff_in_TotalNativeLanguages) ####IS THIS AS FACTOR OR AS NUMERIC?

```
## Data wrangling

To have easier interpretation of the data, we will center the generation number. With that, our estimate of the intercept will always apply for generation in the middle. Otherwise, it would be estimate for the first generation

```{r}

data$generation_number_c <- data$generation_number - median(range(data$generation_number))
```
Below, we adjust all of the entropy data by a constant (half the value of the minimum value), so we don't have any zeros in our data. (This is reasonable because zero entropy doesn't make sense anyway).
```{r}
c <- min(data$total_body_entropy2Daggregated[data$total_body_entropy2Daggregated > 0]) / 2 # decide on a constant
data$entropy_shifted <- data$total_body_entropy2Daggregated + c  # Shift all values
c2 <-min(data$total_body_variability2Daggregated[data$total_body_variability2Daggregated > 0]) / 2
data$variability_shifted <- data$total_body_variability2Daggregated + c2
```

# Question One, part one: Generation v gesture space

Below, we start to focus on our first question (part one: gesture space)

1) We hypothesize that gesture space decreases as a function of generation.
First, we look at the distribution of the gesture space data.

Results: It seems to be that there is a lognormal distribution.
```{r}
hist(data$total_gesture_space2D)
```

Below, we look at how our first question is answered purely by the data. We plot generation vs total gesture space. 

Result analysis: Based on the graph below, it seems as if there is a tiny tiny increase across generations, but it is very hard to tell if there is any pattern at all.

```{r}

ggplot(data, aes(x = generation_number, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm",formula = y ~ poly(x, 3), color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Generation Number", 
       y = "total_gesture_space2D", 
       title = "Linear Relationship between Generation Number and Gesture space") +
  theme_minimal()

```
```{r}
ggplot(data, aes(x = generation_number, y = total_gesture_space2D, color = factor(Gender))) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(aes(group = factor(Gender)), method = "lm", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "generation", 
       y = "Gesture Space", 
       title = "Linear Relationship between generation and Gesture space") +
  theme_minimal()
```
##We now test four different models to understand relation between generation and gesture space.

## Model 1 - intercept only

This is the model
```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m1.gs <- brm(
  total_gesture_space2D ~ generation_number_c, 
  data = data,
  family = gaussian(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m1.gs <- add_criterion(m1.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m1.gs_R2 <- bayes_R2(m1.gs)

# Save both as objects
saveRDS(m1.gs, file.path(models, "m1.gs.rds"))
saveRDS(m1.gs_R2, file.path(models, "m1.gs_R2.rds"))

beep(5)

```

Here we load the model so that we don't have to run the model again
```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m1.gs <- readRDS(file.path(models, "m1.gs.rds"))
m1.gs_R2 <- readRDS(file.path(models, "m1.gs_R2.rds"))
# Summary
summary(m1.gs)

```
Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m1.gs)
# make some comments of what you see - do you see nice hairy catterpillars?

plot(conditional_effects(m1.gs), points = TRUE)
# also here about what effect you see or not see

pp_check(m1.gs, type = "dens_overlay")
# how well does the model predict future data? It does an okay job. Not great.

pp_check(m1.gs, type = "error_scatter_avg")
# how are the values correlated with residual error? High correlatio

m1.gs_R2 
# how much variance the model explains? .8% of the variance

```
Now we are not happy, because for example ppcheck reveals that...

As we inspected the outcome variable, we think that lognormal family could be maybe more appropriate. So we try it 

## Model 2 lognormal

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m2.gs <- brm(
  total_gesture_space2D ~ generation_number_c, 
  data = data,
  family = lognormal(),  # Here we change lognormal
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m2.gs <- add_criterion(m2.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m2.gs_R2 <- bayes_R2(m2.gs)

# Save both as objects
saveRDS(m2.gs, file.path(models, "m2.gs.rds"))
saveRDS(m2.gs_R2, file.path(models, "m2.gs_R2.rds"))


beep(5)

```
 
And we do the same procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m2.gs <- readRDS(file.path(models, "m2.gs.rds"))
m2.gs_R2 <- readRDS(file.path(models, "m2.gs_R2.rds"))


# Summary
summary(m2.gs)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m2.gs)
# comments

plot(conditional_effects(m2.gs), points = TRUE)
# We barely see any slope.

pp_check(m2.gs, type = "dens_overlay")
# Not a bad overlay

pp_check(m2.gs, type = "error_scatter_avg")
# There is a linear correlation it seems which is not good.

m2.gs_R2
# It only explains .7 percent of the data! Yikes!

```

We are still not acknowledging some structures of the data to the model. There is some variance coming from XYZ. So we want to add it into the model too

## Model 3 - lognormal with random effects

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m3.gs <- brm(
  total_gesture_space2D ~ generation_number_c + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 4000, warmup = 2000, cores = 4
)

# Add criterions for later diagnostics
m3.gs <- add_criterion(m3.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m3.gs_R2 <- bayes_R2(m3.gs)

# Save both as objects
saveRDS(m3.gs, file.path(models, "m3.gs.rds"))
saveRDS(m3.gs_R2, file.path(models, "m3.gs_R2.rds"))


beep(5)

```

And once again we do all over the whole procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m3.gs <- readRDS(file.path(models, "m3.gs.rds"))
m3.gs_R2 <- readRDS(file.path(models, "m3.gs_R2.rds"))


# Summary
summary(m3.gs)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m3.gs)

# I am unclear what I am supposed to see here

plot(conditional_effects(m3.gs), points = TRUE)
# I see almost no slope

pp_check(m3.gs, type = "dens_overlay")
# looks pretty accurate?

pp_check(m3.gs, type = "error_scatter_avg")
# some correlation, but it also looks like a blob a bit which is good!


m3.gs_R2
# explains 68.18 of the variance!

```
# Here we convert the fixed effects back to normal scale
```{r}
post <- as_draws_df(m3.gs)

# Extract only fixed effects (intercept and predictors)
post_fixed <- post %>% 
  select(starts_with("b_")) %>%
  rename_with(~ gsub("^b_", "", .))  # remove b_ prefix

# Convert to response scale by exponentiating
post_fixed_exp <- exp(post_fixed)

summary_df <- post_fixed_exp %>%
  pivot_longer(everything(), names_to = "predictor", values_to = "estimate") %>%
  group_by(predictor) %>%
  summarise(
    mean = mean(estimate),
    SE = sd(estimate),
    lower = quantile(estimate, 0.025),
    upper = quantile(estimate, 0.975),
    pdlarger = max(mean(estimate > 1),
    pdsmaller = mean(estimate < 1))  # posterior prob. of direction
  )

summary_df
```
# Here we convert the random effects back to normal scale.
```{r}
post_random <- post %>% 
  select(starts_with("r_")) %>%
  rename_with(~ gsub("^r_", "", .))  # remove r_ prefix

# Convert to response scale by exponentiating (if applicable)
post_random_exp <- exp(post_random)

# Summarize random effects
summary_random_df <- post_random_exp %>%
  pivot_longer(everything(), names_to = "group", values_to = "estimate") %>%
  group_by(group) %>%
  summarise(
    mean = mean(estimate),
    SE = sd(estimate),
    lower = quantile(estimate, 0.025),
    upper = quantile(estimate, 0.975),
    pdlarger = mean(estimate > 1),
    pdsmaller = mean(estimate < 1)  # posterior prob. of direction
  )
summary_random_df
```
## MODEL FOUR: (Investigate other fixed effects)
In this model, we investigate the effects of Age, Extroversion, and Gender. Thus, we begin by plotting the relationships between them. 

Age:
```{r}

ggplot(data, aes(x = Age, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Age", 
       y = "Gesture Space", 
       title = "Linear Relationship between Age and Gesture space") +
  theme_minimal()

```
Extroversion
```{r}

ggplot(data, aes(x = Extroversion, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Extroversion", 
       y = "Gesture Space", 
       title = "Linear Relationship between Extroversion and Gesture space") +
  theme_minimal()

```
Gender
```{r}

ggplot(data, aes(x = Gender, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Gender", 
       y = "Gesture Space", 
       title = "Linear Relationship between Extroversion and Gesture space") +
  theme_minimal()

```
#Models 4:
```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}
# Z score demographic variables:
data$z_score_Age <- (data$Age - mean(data$Age, na.rm = TRUE)) / sd(data$Age, na.rm = TRUE)
data$z_score_Extroversion <- (data$Age - mean(data$Extroversion, na.rm = TRUE)) / sd(data$Extroversion, na.rm = TRUE)
```
```{r}

ggplot(data, aes(x = z_score_Age, y = total_gesture_space2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Age", 
       y = "Gesture Space", 
       title = "Linear Relationship between Age and Gesture space") +
  theme_minimal()

```
```{r}

m4.gs <- brm(
  total_gesture_space2D ~ generation_number_c + Age + Extroversion + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4, 
  control = list(adapt_delta = 0.999, max_treedepth = 12)
)

# Add criterions for later diagnostics
m4.gs <- add_criterion(m4.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m4.gs_R2 <- bayes_R2(m4.gs)

# Save both as objects
saveRDS(m4.gs, file.path(models, "m4.gs.rds"))
saveRDS(m4.gs_R2, file.path(models, "m4.gs_R2.rds"))


beep(5)

m4.gs <- readRDS(file.path(models, "m4.gs.rds"))
m4.gs_R2 <- readRDS(file.path(models, "m4.gs_R2.rds"))


# Summary
summary(m4.gs)

```
Let's also check visuals
```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m4.gs)

# not totally sure how to interpret. 

plot(conditional_effects(m4.gs), points = TRUE)
#

pp_check(m4.gs, type = "dens_overlay")
# 

pp_check(m4.gs, type = "error_scatter_avg")
# 


m4.gs_R2
# explains 66.28 percent of the variance
```
# MODEL FIVE: includes gender and num of languages as a fixed variable. Also has save pars as true and control  
```{r}
m5.gs <- brm(
  total_gesture_space2D ~ generation_number_c + Age + Extroversion + Gender +NumOfNativeLanguages + (1 | participantID) + (1 | gesture) + (1 | chainID), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4,
  save_pars = save_pars(all = TRUE),
  control = list(adapt_delta = 0.99)
)

# Add criterions for later diagnostics
m5.gs <- add_criterion(m5.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m5.gs_R2 <- bayes_R2(m4.gs)

# Save both as objects
saveRDS(m5.gs, file.path(models, "m5.gs.rds"))
saveRDS(m5.gs_R2, file.path(models, "m5.gs_R2.rds"))

beep(5)

m5.gs <- readRDS(file.path(models, "m5.gs.rds"))
m5.gs_R2 <- readRDS(file.path(models, "m5.gs_R2.rds"))


# Summary
summary(m5.gs)

```

```{r}
plot(m5.gs)
# make some comments of what you see - do you see nice hairy catterpillars?

plot(conditional_effects(m5.gs), points = TRUE)
# also here about what effect you see or not see

pp_check(m5.gs, type = "dens_overlay")
# how well does the model predict future data? well. 

pp_check(m5.gs, type = "error_scatter_avg")
# how are the values correlated with residual error? not really correlated too much

m5.gs_R2
# accounts for 66.2 percent of the variance
```
# UNDERSTANDING INTERACTION
```{r}
ggplot(data, aes(x = Extroversion, y = total_distance_c, color = factor(diff_in_gender))) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(aes(group = factor(diff_in_gender)), method = "lm", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Age", 
       y = "Gesture Space", 
       title = "Linear Relationship between Age and Gesture space") +
  theme_minimal()
```
# Question One, part two: Generation v Entropy
Here we continue our first question (part two: entropy)

1) We hypothesize that entropy decreases as a function of generation.

Here we look at how the question is answered purely by the data:

```{r}

ggplot(data, aes(x = generation_number, y = entropy_shifted)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Generation Number", 
       y = "entropy_shifted", 
       title = "Linear Relationship between Generation Number and Gesture space") +
  theme_minimal()

```

Look at the distribution of the outcome:

```{r}

hist(data$entropy_shifted)

```

## Model 1 - intercept only

This is the model
```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m1.et <- brm(
  entropy_shifted ~ generation_number_c, 
  data = data,
  family = gaussian(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m1.et <- add_criterion(m1.et, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m1.et_R2 <- bayes_R2(m1.et)

# Save both as objects
saveRDS(m1.et, file.path(models, "m1.et.rds"))
saveRDS(m1.et_R2, file.path(models, "m1.et_R2.rds"))

beep(5)

```

Here we load the model so that we don't have to run the model again
```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m1.et <- readRDS(file.path(models, "m1.et.rds"))
m1.et_R2 <- readRDS(file.path(models, "m1.et_R2.rds"))


# Summary
summary(m1.et)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m1.et)
# make some comments of what you see - do you see nice hairy catterpillars?

plot(conditional_effects(m1.et), points = TRUE)
# also here about what effect you see or not see

pp_check(m1.et, type = "dens_overlay")
# how well does the model predict future data? meh

pp_check(m1.et, type = "error_scatter_avg")
# how are the values correlated with residual error? lots of correlation

m1.et_R2
# how much variance the model explains? .01

```
Now we are not happy, because for example ppcheck reveals that...

As we inspected the outcome variable, we think that lognormal family could be maybe more appropriate. So we try it 

## Model 2 lognormal

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m2.et <- brm(
  entropy_shifted ~ generation_number_c, 
  data = data,
  family = lognormal(),  # Here we change lognormal
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)


# Add criterions for later diagnostics
m2.et <- add_criterion(m2.et, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m2.et_R2 <- bayes_R2(m2.et)

# Save both as objects
saveRDS(m2.et, file.path(models, "m2.et.rds"))
saveRDS(m2.et_R2, file.path(models, "m2.et_R2.rds"))


beep(5)

```
 
And we do the same procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m2.et <- readRDS(file.path(models, "m2.et.rds"))
m2.et_R2 <- readRDS(file.path(models, "m2.et_R2.rds"))


# Summary
summary(m2.et)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m2.et)
# comments

plot(conditional_effects(m2.et), points = TRUE)
# slgihtly negative

pp_check(m2.et, type = "dens_overlay")
# comments # meh

pp_check(m2.et, type = "error_scatter_avg")
# comments very correlatied

m2.et_R2
# comments 1.3%

```

We are still not acknowledging some structures of the data to the model. There is some variance coming from XYZ. So we want to add it into the model too

## Model 3 - lognormal with random effects 

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m3.et <- brm(
  entropy_shifted ~ generation_number_c + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal,  # (I tested gamma and gaussian and they didn't give me better results)
  chains = 4, iter = 4000, warmup = 2000, cores = 4
)

# Add criterions for later diagnostics
m3.et <- add_criterion(m3.et, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m3.et_R2 <- bayes_R2(m3.et)

# Save both as objects
saveRDS(m3.et, file.path(models, "m3.et.rds"))
saveRDS(m3.et_R2, file.path(models, "m3.et_R2.rds"))


beep(5)

```

And once again we do all over the whole procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m3.et <- readRDS(file.path(models, "m3.et.rds"))
m3.et_R2 <- readRDS(file.path(models, "m3.et_R2.rds"))


# Summary
summary(m3.et)

```



Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m3.et)

#

plot(conditional_effects(m3.et), points = TRUE)
#

pp_check(m3.et, type = "dens_overlay")
# meh

pp_check(m3.et, type = "error_scatter_avg")
# not so correlated


m3.et_R2
#  explains 56.6% of the model

```
# Here we convert the effects back to normal scale
```{r}
post <- as_draws_df(m3.et)

# Extract only fixed effects (intercept and predictors)
post_fixed <- post %>% 
  select(starts_with("b_")) %>%
  rename_with(~ gsub("^b_", "", .))  # remove b_ prefix

# Convert to response scale by exponentiating
post_fixed_exp <- exp(post_fixed)

summary_df <- post_fixed_exp %>%
  pivot_longer(everything(), names_to = "predictor", values_to = "estimate") %>%
  group_by(predictor) %>%
  summarise(
    mean = mean(estimate),
    SE = sd(estimate),
    lower = quantile(estimate, 0.025),
    upper = quantile(estimate, 0.975),
    pdlarger = max(mean(estimate > 1),
    pdsmaller = mean(estimate < 1))  # posterior prob. of direction
  )

summary_df
```
## MODEL FOUR: (Investigate other fixed effects)
In this model, we investigate the effects of Age, Extroversion, and Gender. Thus, we begin by plotting the relationships between them. 

Age:
```{r}

ggplot(data, aes(x = Age, y = entropy_shifted)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Age", 
       y = "Entropy", 
       title = "Linear Relationship between Age and Entropy") +
  theme_minimal()

```
Extroversion
```{r}

ggplot(data, aes(x = Extroversion, y = entropy_shifted)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Extroversion", 
       y = "Entropy", 
       title = "Linear Relationship between Extroversion and Entropy") +
  theme_minimal()

```
Gender
```{r}

ggplot(data, aes(x = Gender, y = entropy_shifted)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Gender", 
       y = "Entropy", 
       title = "Linear Relationship between Gender and Entropy") +
  theme_minimal()

```

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m4.et <- brm(
  entropy_shifted ~ generation_number_c + Age + Extroversion + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m4.et <- add_criterion(m4.et, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m4.et_R2 <- bayes_R2(m4.et)

# Save both as objects
saveRDS(m4.et, file.path(models, "m4.et.rds"))

saveRDS(m4.et_R2, file.path(models, "m4.et_R2.rds"))


beep(5)

m4.et <- readRDS(file.path(models, "m4.et.rds"))
m4.et_R2 <- readRDS(file.path(models, "m4.et_R2.rds"))


# Summary
summary(m4.et)

```
Let's also check visuals
```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m4.et)

#

plot(conditional_effects(m4.et), points = TRUE)
#

pp_check(m4.et, type = "dens_overlay")
# meh

pp_check(m4.et, type = "error_scatter_avg")
# some correlation


m4.et_R2
# explains 55% of the data
```
## Here we convert it back to regular scale
```{r}
post <- as_draws_df(m4.et)

# Extract only fixed effects (intercept and predictors)
post_fixed <- post %>% 
  select(starts_with("b_")) %>%
  rename_with(~ gsub("^b_", "", .))  # remove b_ prefix

# Convert to response scale by exponentiating
post_fixed_exp <- exp(post_fixed)

summary_df <- post_fixed_exp %>%
  pivot_longer(everything(), names_to = "predictor", values_to = "estimate") %>%
  group_by(predictor) %>%
  summarise(
    mean = mean(estimate),
    SE = sd(estimate),
    lower = quantile(estimate, 0.025),
    upper = quantile(estimate, 0.975),
    pdlarger = max(mean(estimate > 1),
    pdsmaller = mean(estimate < 1))  # posterior prob. of direction
  )

summary_df
```
# Question One, part three: Generation v Temporal Variability
Here we continue our first question (part three: temporal variability)

1) We hypothesize that temporal variability decreases as a function of generation.

Here we look at how the question is answered purely by the data:

```{r}

ggplot(data, aes(x = generation_number, y = total_body_variability2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Generation Number", 
       y = "Temporal Variability", 
       title = "Linear Relationship between Generation Number and Temporal Variability") +
  theme_minimal()

```

Look at the distribution of the outcome:

```{r}

hist(data$total_body_variability2D)

```

## Model 1 - intercept only

This is the model
```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m1.tv <- brm(
  variability_shifted ~ generation_number_c, 
  data = data,
  family = gaussian(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m1.tv <- add_criterion(m1.et, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m1.tv_R2 <- bayes_R2(m1.tv)

# Save both as objects
saveRDS(m1.tv, file.path(models, "m1.tv.rds"))
saveRDS(m1.tv_R2, file.path(models, "m1.tv_R2.rds"))

beep(5)

```

Here we load the model so that we don't have to run the model again
```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m1.tv <- readRDS(file.path(models, "m1.tv.rds"))
m1.tv_R2 <- readRDS(file.path(models, "m1.tv_R2.rds"))


# Summary
summary(m1.tv)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m1.tv)
# make some comments of what you see - do you see nice hairy catterpillars?

plot(conditional_effects(m1.tv), points = TRUE)
# I do not see any effect of generation on temporal variability. Slight decrease

pp_check(m1.tv, type = "dens_overlay")
# Does not predict future data well. 

pp_check(m1.tv, type = "error_scatter_avg")
# High correlation between values and residentual error which is bad.

m1.tv_R2
# It only explains 1.1% of the variance

```
As we inspected the outcome variable, we think that lognormal family could be maybe more appropriate. So we try it 

## Model 2 lognormal

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m2.tv <- brm(
  variability_shifted ~ generation_number_c, 
  data = data,
  family = lognormal(),  # Here we change lognormal
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m2.tv <- add_criterion(m2.tv, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m2.tv_R2 <- bayes_R2(m2.tv)

# Save both as objects
saveRDS(m2.tv, file.path(models, "m2.tv.rds"))
saveRDS(m2.tv_R2, file.path(models, "m2.tv_R2.rds"))


beep(5)

```
 
And we do the same procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m2.tv <- readRDS(file.path(models, "m2.tv.rds"))
m2.tv_R2 <- readRDS(file.path(models, "m2.tv_R2.rds"))


# Summary
summary(m2.tv)

```


Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m2.tv)
# comments

plot(conditional_effects(m2.tv), points = TRUE)
# almost no conditional effects

pp_check(m2.tv, type = "dens_overlay")
# seems better

pp_check(m2.tv, type = "error_scatter_avg")
# still has a correlation

m2.tv_R2
# accounts for 2 percent of the variance.

```

We are still not acknowledging some structures of the data to the model. There is some variance coming from XYZ. So we want to add it into the model too

## Model 3 - lognormal with random effects

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m3.tv <- brm(
  variability_shifted ~ generation_number_c + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 4000, warmup = 2000, cores = 4
)

# Add criterions for later diagnostics
m3.tv <- add_criterion(m3.tv, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m3.tv_R2 <- bayes_R2(m3.tv)

# Save both as objects
saveRDS(m3.tv, file.path(models, "m3.tv.rds"))
saveRDS(m3.tv_R2, file.path(models, "m3.tv_R2.rds"))


beep(5)

```

And once again we do all over the whole procedure

```{r h1.m1 sum, echo=TRUE, message=FALSE, warning=FALSE}

m3.tv <- readRDS(file.path(models, "m3.tv.rds"))
m3.tv_R2 <- readRDS(file.path(models, "m3.tv_R2.rds"))


# Summary
summary(m3.tv)

```



Let's also check the visuals

```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m3.tv)

#

plot(conditional_effects(m3.tv), points = TRUE)
# almost no effect

pp_check(m3.tv, type = "dens_overlay")
# seems like a pretty good overlay

pp_check(m3.tv, type = "error_scatter_avg")
# there is some correlation but also lots of blobs

m3.tv_R2
# explains 32 percent of the variance. Yikes

```
## MODEL FOUR: (Investigate other fixed effects)
In this model, we investigate the effects of Age, Extroversion, and Gender. Thus, we begin by plotting the relationships between them. 

Age:
```{r}

ggplot(data, aes(x = Age, y = total_body_variability2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Age", 
       y = "Temporal Variability", 
       title = "Linear Relationship between Age and Gesture space") +
  theme_minimal()

```
Extroversion
```{r}

ggplot(data, aes(x = Extroversion, y = total_body_variability2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Extroversion", 
       y = "Temporal Variability", 
       title = "Linear Relationship between Extroversion and Temporal Variability") +
  theme_minimal()

```
Gender
```{r}

ggplot(data, aes(x = Gender, y = total_body_variability2D)) +
  geom_point(alpha = 0.5) +  # Scatter plot with transparency
  geom_smooth(method = "lm", color = "blue", se = TRUE) +  # Linear trend line with confidence interval
  labs(x = "Gender", 
       y = "Temporal Variability", 
       title = "Linear Relationship between Gender and Temporal Variability") +
  theme_minimal()

hist(data$variability_shifted)

```

```{r h1.m1 model, eval=FALSE, message=FALSE, warning=FALSE}

m4.tv <- brm(
  variability_shifted ~ generation_number_c + Age + Extroversion + (1 | participantID) + (1 | gesture) + (1 | chain_number), 
  data = data,
  family = lognormal(),  # Normal distribution for continuous entropy
  chains = 4, iter = 2000, warmup = 1000, cores = 4
)

# Add criterions for later diagnostics
m4.tv <- add_criterion(m4.gs, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
m4.tv_R2 <- bayes_R2(m4.tv)

# Save both as objects
saveRDS(m4.tv, file.path(models, "m4.tv.rds"))
saveRDS(m4.tv_R2, file.path(models, "m4.tv_R2.rds"))


beep(5)

m4.tv <- readRDS(file.path(models, "m4.tv.rds"))
m4.tv_R2 <- readRDS(file.path(models, "m4.tv_R2.rds"))


# Summary
summary(m4.tv)

```
Let's also check visuals
```{r h1.m1 check, echo=TRUE, message=FALSE, warning=FALSE}

plot(m4.tv)

#

plot(conditional_effects(m4.tv), points = TRUE)
# almost no correlation

pp_check(m4.tv, type = "dens_overlay")
# good overlay

pp_check(m4.tv, type = "error_scatter_avg")
# some correlation but also some blobbing


m4.tv_R2
# explains 66 percent of the variance.
```
